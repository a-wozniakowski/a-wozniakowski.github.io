---
layout: post
title: Gaussian mixture model
date: 2024-03-17
description: Derivation of a Gaussian mixture model
tags: expectation-maximization EM gaussian-mixture-model GMM
categories: machine-learning statistics
related_posts: false
featured: true
---

In this exposition, we formulate the expectation-maximization (EM) algorithm
and employ it to derive the Gaussian mixture model (GMM).

- To achieve this, we prove a form of Jensen's inequality via induction
  and apply it to obtain the evidence lower bound (ELBO), where the latter
  plays a broad role in modern machine learning.
- In turn, we see how the interplay between the log-likelihood and
  the evidence lower bound naturally leads us to a derivation of the
  expectation-maximization algorithm:
  - A point estimation method that initializes model parameters
    then alternates between an E-step and M-step until convergence
    to the maximum likelihood estimate of the model parameters.
- Subsequently, we employ the expectation-maximization algorithm with the
  Gaussian mixture model assumptions to derive the Gaussian mixture model
  learning algorithm as a special case.
  - Lastly, in the appendix, we delve into the intricacies of the gradient
    computations, which underly the M-step update to the covariance matrices.
    Notedly, this essential step is often left to the reader.

Additionally, we note that there is an implementation of the Gaussian mixture model
in the ```ml/mixture``` directory of the
[ml_playground](https://github.com/a-wozniakowski/ml_playground/tree/main)
repository, which inspired a new feature in the
[scikit-learn](https://github.com/scikit-learn/scikit-learn/issues/28492)
repository.

<h2>Introduction</h2>

Our formulation of the expectation-maximization (EM) algorithm begins with
a random sample of $$m$$ examples, denoted as an $$m$$-tuple:

\begin{equation}
( X^{(1)},\dots,X^{(m)} ),
\label{examples}
\end{equation}

where each $$X^{(i)} = x^{(i)} \in \mathbb{R}^{n}$$ represents an independent
and identically distributed random vector from an unknown probability
distribution. Correspondingly, the log-likelihood $$\ell$$ is then expressed as:

\begin{equation}
\ell (\theta) = \ln (f_{X_{1},\dots,X_{m}}(x^{(1)},\dots,x^{(m)})) = \ln(\prod_{i=1}^{m} f(x^{(i)})) = \sum_{i=1}^{m} \ln (f(x^{(i)})),
\label{log-likelihood}
\end{equation}

where $$\theta$$ bundles all model parameters, $$f_{X_{1},\dots,X_{m}}$$ denotes the
joint density function, and $$f_{X}=f$$ denotes the population density function.

Next, we posit a discrete random variable $$Z$$ with $$k$$ mutually exclusive outcomes, and we
refer to the random variable as a latent variable. Subsequently, we modify the right-hand-side
of Eq. \ref{log-likelihood} such that

\begin{equation}
\sum_{i=1}^{m} \ln (f(x^{(i)})) = \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j)),
\label{marginalization}
\end{equation}

where $$f_{X,Z}$$ denotes the joint density function. 

In turn, we seek to modify the right-hand-side of Eq. \ref{marginalization} such that we can
leverage a form of Jensen's inequality and obtain the evidence lower bound (ELBO) on the
log-likelihood.

<h2>Evidence lower bound</h2>

To obtain the evidence lower bound on the log-likelihood, we begin with induction to derive a
form of Jensen's inequality. Namely, we suppose the base case states that $$g$$ is a convex
function

$$
P(2):\ g(\lambda u_{1} + (1 - \lambda) u_2) \leq \lambda g(u_{1}) + (1 - \lambda) g(u_{2}),
$$

where $$\lambda \in [ 0, 1 ].$$ Next, in the induction step, we suppose the induction hypothesis
states that

$$
P(k):\ g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \leq \sum_{j=1}^{k} \lambda_{j} g(u_{j}),
$$

where $$k \geq 2,$$ $$\lambda_{j} \geq 0$$ for every convex combination coefficient, and
$$\sum_{j=1}^{k} \lambda_{j} = 1.$$ 

Then, we have that

$$
\begin{align*}
P(k+1):\ g(\sum_{j=1}^{k+1} \lambda_{j} u_{j}) 
&= g(\lambda_{k+1} u_{k+1} + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \frac{\lambda_{j}}{1 - \lambda_{k+1}} u_{j}) \\
&= g(\lambda_{k+1} u_{k+1} + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \mu_{j} u_{j}) \\
&\leq \lambda_{k+1} g(u_{k+1}) + (1 - \lambda_{k+1}) g(\sum_{j=1}^{k} \mu_{j} u_{j}) \\
&\leq \lambda_{k+1} g(u_{k+1}) + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \mu_{j} g(u_{j}) \\
&= \lambda_{k+1} g(u_{k+1}) + \sum_{j=1}^{k} \lambda_{j} g(u_{j}) \\
&= \sum_{j=1}^{k+1} \lambda_{j} g(u_{j})
\end{align*},
$$

where $$\mu_{j} = \frac{\lambda_{j}}{1 - \lambda_{k+1}}$$ for every convex combination coefficient
from $$1,\dots,k,$$ the first inequality follows from the base case $$P(2),$$ and the second inequality
follows from the induction hypothesis $$P(k).$$ Therefore, we observe that

$$
g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \leq \sum_{j=1}^{k} \lambda_{j} g(u_{j})
$$

for $$k \geq 2;$$ and, in the case that $$g$$ is a concave function, we have that

\begin{equation}
g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \geq \sum_{j=1}^{k} \lambda_{j} g(u_{j})
\label{jensen}
\end{equation}

for $$k \geq 2,$$ which is the form of the inequality that we want in our derivation.

Correspondingly, we modify the right-hand-side of Eq. \ref{marginalization} such that

$$
\begin{align}
\sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j)) 
&= \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} w_{j}^{(i)} \frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}) \label{before_jensen}\\
&\geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}) \label{after_jensen}
\end{align}
$$

where the weight $$w_{j}^{(i)}$$ denotes a convex combination coefficient and the inequality 
follows from Ineq. \ref{jensen}. That is, we have that the log-likelihood is bounded from below
by the evidence lower bound

$$
\ell (\theta) \geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}).
$$

<h2>Expectation-maximization algorithm</h2>

To derive the expectation-maximization algorithm, we initialize the model parameters such that
$$\theta_{t} = \theta_{0}.$$ Subsequently, in iteration $$t+1,$$ we perform an E-step, then an
M-step, to obtain model parameters $$\theta_{t+1},$$ which monotonically increase the log-likelihood,
i.e., $$\ell (\theta_{t+1}) \geq  \ell (\theta_{t}).$$

That is, initially, in iteration $$t+1,$$ we seek weights such that the log-likelihood is equal
to the evidence lower bound at the current model parameters.

<h4>E-step:</h4>

To find such weights, we compare the right-hand-side of Eq. \ref{before_jensen} with the
right-hand-side of Ineq. \ref{after_jensen}; and from that comparison, we deduce that the
quotient $$\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}$$ must be equal to a constant $$c.$$ 
Accordingly, we have that

$$
c = c \sum_{j=1}^{k} w_{j}^{(i)} = \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j) = f(x^{(i)}),
$$ 

which implies that for each $$i=1,\dots,m$$ and $$j=1,\dots,k$$

\begin{equation}
w_{j}^{(i)} = f_{Z \mid X}(j \mid x^{(i)} ; \theta_{t}) 
= \frac{f_{Z}(j ; \theta_{t}) f_{X \mid Z}(x^{(i)} \mid j ; \theta_{t})}{\sum_{u=1}^{k} f_{Z}(u ; \theta_{t}) f_{X \mid Z}(x^{(i)} \mid u ; \theta_{t})},
\label{E-step}
\end{equation}

where we make the parameterization explicit. 

Hence, we observe that the log-likelihood is equal to the evidence lower bound at the current
model parameters

\begin{equation}
\ell (\theta_{t}) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(f(x^{(i)} ; \theta_{t})),
\label{current-log-likelihood}
\end{equation}

when we fix the weights as in Eq. \ref{E-step}, where we, again, make the parameterization explicit.
In turn, we seek model parameters, which maximally increase the log-likelihood with respect to
Eq. \ref{current-log-likelihood}.

<h4>M-step:</h4>

To find such model parameters, we aim to maximize the following difference

$$
\begin{align*}
\ell (\theta) - \ell (\theta_{t}) 
&\geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} (\ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{w_{j}^{(i)}}) - \ln(f(x^{(i)} ; \theta_{t}))) \\
&= \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})})
\end{align*}.
$$

Correspondingly, we modify the inequality such that

$$
\begin{align*}
\ell (\theta) 
&\geq \ell (\theta_{t}) + \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})}) \\
& = \ell (\theta_{t}) + S(\theta, \theta_{t}) \\
& = T(\theta, \theta_{t}),
\end{align*}
$$

where we introduce functions

$$
S(\theta, \theta_{t}) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})})
$$

and

$$
T(\theta, \theta_{t}) = \ell (\theta_{t}) + S(\theta, \theta_{t})
$$

which satisfy $$ S(\theta_{t}, \theta_{t}) = 0$$ and $$T(\theta_{t}, \theta_{t}) = \ell (\theta_{t}).$$

Thus, if we find $$\theta$$ such that

$$
T(\theta, \theta_{t}) \geq T(\theta_{t}, \theta_{t}),
$$

then we have that

$$
\ell (\theta) \geq T(\theta, \theta_{t}) \geq T(\theta_{t}, \theta_{t}) = \ell (\theta_{t}),
$$

which implies that we monotonicly increase the log-likelihood with respect to
Eq. \ref{current-log-likelihood}. Trivially, to do so, we pick 

$$
\theta_{t+1} = \theta = \theta_{t},
$$

where we saturate the inequality. 

However, our goal is to maximally increase the log-likelihood with respect to
Eq. \ref{current-log-likelihood}. Therefore, we must solve the following
optimization problem to obtain the desired model parameters


\begin{equation}
\theta_{t+1} = \underset{\theta}{\operatorname{argmax}} T(\theta, \theta_{t}) 
= \underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (f_{X,Z}(x^{(i)}, j ; \theta)).
\label{M-step}
\end{equation}

Subsequently, we continue to iterate, where we alternate between the E-step and M-step
until we reach some convergence criterion, i.e., we find the maximum likelihood estimate
of the model parameters.

<h2>Gaussian mixture model</h2>

To derive the Gaussian mixture model, we assume that the latent variable 

$$
Z \sim Categorical (\phi),
$$

where a categorical random variable is a special case of a multinomial random variable with a single
trial and $$k$$ mutually exclusive outcomes. Moreover, the categorical random variable has probability mass
function

$$
f_{Z}(z) = \prod_{j=1}^{k} \phi_{j}^{\mathcal{I} \{ z=j \}},
$$

where $$\mathcal{I}$$ denotes the indicator function. Next, we assume that given the latent variable $$Z=j,$$
we have that 

$$
X | Z = j \sim \mathcal{N}(\mu_{j}, \Sigma_{j}),
$$

where the conditional Gaussian random vector has conditional probability density function 

$$
f_{X|Z} (x | j) = \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{j}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_{j})^{t}\Sigma_{j}^{-1}(x-\mu_{j})}.
$$

Correspondingly, we initialize all model parameters, then we consider iteration $$t+1.$$ 

<h4>E-step:</h4>

Namely, we set the weights such that

$$
w_{j}^{(i)} = 
\frac{\phi_{j} \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{j}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{j})^{t}\Sigma_{j}^{-1}(x^{(i)}-\mu_{j})}}{\sum_{u=1}^{k} \phi_{u} \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}}
$$

for each $$i=1,\dots,m$$ and $$j=1,\dots,k,$$ which follows from Eq. \ref{E-step}. 

<h4>M-step:</h4>

Then, we set the  model parameters such that they satisfy Eq. \ref{M-step}.
To do so, in the case of $$\phi,$$ we posit the auxillary function

$$
h(\phi_{1},\dots,\phi_{k},\lambda) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1),
$$

where $$\lambda$$ denotes the Lagrange multiplier and we only include the terms in
Eq. \ref{M-step}, which depend on $$\phi_{1},\dots,\phi_{k}.$$ Subsequently, we
apply the Lagrange multiplier theorem to obtain

$$
\begin{align*}
0 &= \frac{\partial}{\partial \phi_{1}} (\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1)) = \sum_{i=1}^{m} w_{1}^{(i)} \frac{1}{\phi_{1}} - \lambda\\
&\vdots \\
0 &= \frac{\partial}{\partial \phi_{k}} (\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1)) = \sum_{i=1}^{m} w_{k}^{(i)} \frac{1}{\phi_{k}} - \lambda \\
0 &= \frac{\partial}{\partial \lambda} \lambda (\sum_{j=1}^{k} \phi_{j} - 1) = \sum_{j=1}^{k} \phi_{j} - 1
\end{align*}
$$

which implies that 

$$
m = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} = \lambda \sum_{j=1}^{k} \phi_{j} = \lambda.
$$

Therefore, we have that

$$
\hat{\phi}_{j} = \frac{\sum_{i=1}^{m} w_{j}^{(i)}}{m}
$$

for $$j=1,\dots,k.$$

To do so, in the case of $$\mu_{j},$$ we consider the critical point of the gradient of the summand in Eq. \ref{M-step}
at the $$j$$th mean

$$
0 = \nabla_{\mu_{j}} \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \ln(\frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}),
$$

where we only include the terms in the summand, which depend on $$\mu_{j}.$$ Subsequently, we simplify the equation to obtain

$$
\begin{align*}
0 &= \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \nabla_{\mu_{j}} -\frac{1}{2} (x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u}) \\
&= \Sigma_{j}^{-1} \sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j}) \\
\end{align*}
$$

which implies that 

$$
0 = \sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j})
$$

since the $$j$$th covariance matrix is invertible. Therefore, we have that

$$
\hat{\mu}_{j} = \frac{\sum_{i=1}^{m} w^{(i)}_{j} x^{(i)}}{\sum_{i=1}^{m} w^{(i)}_{j}}
$$

for $$j=1,\dots,k.$$

To do so, in the case of $$\Sigma_{j},$$ we consider the critical point of the gradient of the summand in Eq. \ref{M-step}
at the $$j$$th covariance matrix

$$
0 = \nabla_{\Sigma_{j}} \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \ln(\frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}),
$$

where we only include the terms in the summand, which depend on $$\Sigma_{j}.$$ Subsequently, we simplify the equation
to obtain

$$
0 = \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \nabla_{\Sigma_{j}} -\frac{1}{2} (\ln(|\Sigma_{u}|) + (x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})),
$$

which implies that

$$
\begin{align*}
0 &= \sum_{i=1}^{m} \sum_{u=1}^{k} \mathcal{I} \{ j=u \} w^{(i)}_{u} (\Sigma_{u}^{-1} - \Sigma_{u}^{-1} (x^{(i)} - \mu_{u}) (x^{(i)} - \mu_{u})^{t} \Sigma_{u}^{-1}) \\
&= \sum_{i=1}^{m} w^{(i)}_{j} (\Sigma_{j}^{-1} - \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t} \Sigma_{j}^{-1})
\end{align*},
$$

where we employ Eq. \ref{gradient-determinant} and Eq. \ref{gradient-quadratic-form} from the Appendix to perform the
gradient computations.

Then, we left and right multiply by the $$j$$th covariance matrix to obtain

$$
0 = \sum_{i=1}^{m} w^{(i)}_{j} (\Sigma_{j} - (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t}),
$$

Therefore, we have that

$$
\hat{\Sigma}_{j} = \frac{\sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t}}{\sum_{i=1}^{m} w^{(i)}_{j}}
$$

for $$j=1,\dots,k,$$ which concludes the derivation of the Gaussian mixture model learning algorithm.

Below, we delve into the intricacies of the gradient computations in Eq. \ref{gradient-determinant} and Eq. \ref{gradient-quadratic-form},
which underly the update to the covariance matrices.

<h2>Appendix: Gradient computations</h2>

First, we want to show that

\begin{equation}
\nabla_{A} \ln(|A|) = A^{-1},
\label{gradient-determinant}
\end{equation}

where $$A \in M_{n \times n} (\mathbb{R})$$ denotes an invertible symmetric matrix. To do so, we begin with

$$
\frac{\partial}{\partial A_{ij}} \ln(|A|) = \frac{1}{|A|} \frac{\partial}{\partial A_{ij}} |A| = \frac{1}{|A|} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} A_{ik} C_{ik},
$$

where we employ a cofactor expansion of the determinant on the right-hand-side of the equation. Then, we
have that

$$
\frac{1}{|A|} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} A_{ik} C_{ik} = \frac{C_{ij}}{|A|} = (\frac{C^{t}}{|A|})_{ji} = (A^{-1})_{ji} = ((A^{-1})^{t})_{ij} = ((A^{t})^{-1})_{ij},
$$

which implies that 

$$
\nabla_{A} \ln(|A|) = (A^{t})^{-1}.
$$

Therefore, we obtain Eq. \ref{gradient-determinant}, since $$A=A^{t}.$$

Second, we want to show that

\begin{equation}
\nabla_{A} y^{t} A^{-1} y = -A^{-1} y y^{t} A^{-1}
\label{gradient-quadratic-form}
\end{equation}

where $$A \in M_{n \times n} (\mathbb{R})$$ denotes an invertible symmetric matrix and $$y \in \mathbb{R}^{n}$$ denotes the corresponding vector.
To do so, we begin with

$$
0 = \frac{\partial}{\partial A_{ij}} I_{uv} = \frac{\partial}{\partial A_{ij}} (A^{-1}A)_{uv} = \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} (A^{-1})_{uk} A_{kv},
$$

where $$I$$ denotes the $$n \times n$$ identity matrix. Then, we have that

$$
\begin{align*}
\frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} (A^{-1})_{uk} A_{kv} 
&= \sum_{k=1}^{n} \frac{\partial}{\partial A_{ij}} (A^{-1})_{uk} A_{kv} + \sum_{k=1}^{n} (A^{-1})_{uk} \frac{\partial}{\partial A_{ij}} A_{kv}\\
&= \sum_{k=1}^{n} (\frac{\partial A^{-1}}{\partial A_{ij}})_{uk} A_{kv} + \sum_{k=1}^{n} (A^{-1})_{uk} (E_{ij})_{kv} \\
&= (\frac{\partial A^{-1}}{\partial A_{ij}} A)_{uv} + (A^{-1} E_{ij})_{uv}
\end{align*}
$$

since

$$
\frac{\partial}{\partial A_{ij}} A_{kv} = \mathcal{I} \{ i=k \} \mathcal{I} \{ j=v \} = (E_{ij})_{kv},
$$

where $$E_{ij}$$ denotes the $$n \times n$$ matrix unit with a $$1$$ in the $$(i,j)$$ entry and $$0$$ in every other entry.
Hence, we have that

$$
(\frac{\partial A^{-1}}{\partial A_{ij}} A)_{uv} = -(A^{-1} E_{ij})_{uv},
$$

which implies that

$$
\frac{\partial A^{-1}}{\partial A_{ij}} = - A^{-1} E_{ij} A^{-1}.
$$

In turn, we consider

$$
\frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} = (\frac{\partial A^{-1}}{\partial A_{ij}})_{kl} = - (A^{-1} E_{ij} A^{-1})_{kl} = - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} (E_{ij})_{uv} (A^{-1})_{vl}.
$$

Then, we have that

$$
\begin{align*}
- \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} (E_{ij})_{uv} (A^{-1})_{vl} 
&= - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} \mathcal{I} \{ i=u \} \mathcal{I} \{ j=v \} (A^{-1})_{vl} \\
&= - (A^{-1})_{ki} (A^{-1})_{jl}
\end{align*},
$$

which implies that

$$
\begin{equation}
\frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} = - (A^{-1})_{ki} (A^{-1})_{jl}.
\label{partial-derivative}
\end{equation}
$$

Lastly, we consider 

$$
\frac{\partial}{\partial A_{ij}} y^{t} A^{-1} y = \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} \frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} y_{l} = - \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} (A^{-1})_{ki} (A^{-1})_{jl} y_{l},
$$

which follows from Eq. \ref{partial-derivative}. Consequently, we have that

$$
- \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} (A^{-1})_{ki} (A^{-1})_{jl} y_{l} = - ((A^{-1})^{t} y)_{i} (A^{-1} y)_{j},
$$

which implies that

$$
\nabla_{A} y^{t} A^{-1} y = - (A^{-1})^{t} y y^{t} (A^{-1})^{t} = - (A^{t})^{-1} y y^{t} (A^{t})^{-1}.
$$

Therefore, we obtain Eq. \ref{gradient-quadratic-form}, since $$A=A^{t}.$$
