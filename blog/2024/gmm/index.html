<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gaussian mixture model | Alex Wozniakowski </title> <meta name="author" content="Alex Wozniakowski"> <meta name="description" content="Derivation of a Gaussian mixture model"> <meta name="keywords" content="machine-learning, math, statistics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://a-wozniakowski.github.io/blog/2024/gmm/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alex </span> Wozniakowski </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gaussian mixture model</h1> <p class="post-meta"> March 17, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/expectation-maximization"> <i class="fa-solid fa-hashtag fa-sm"></i> expectation-maximization</a>   <a href="/blog/tag/em"> <i class="fa-solid fa-hashtag fa-sm"></i> EM</a>   <a href="/blog/tag/gaussian-mixture-model"> <i class="fa-solid fa-hashtag fa-sm"></i> gaussian-mixture-model</a>   <a href="/blog/tag/gmm"> <i class="fa-solid fa-hashtag fa-sm"></i> GMM</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/statistics"> <i class="fa-solid fa-tag fa-sm"></i> statistics</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this exposition, we formulate the expectation-maximization (EM) algorithm and employ it to derive the Gaussian mixture model (GMM).</p> <ul> <li>To achieve this, we prove a form of Jensen’s inequality via induction and apply it to obtain the evidence lower bound (ELBO), where the latter plays a broad role in modern machine learning.</li> <li>In turn, we see how the interplay between the log-likelihood and the evidence lower bound naturally leads us to a derivation of the expectation-maximization algorithm: <ul> <li>A point estimation method that initializes model parameters then alternates between an E-step and M-step until convergence to the maximum likelihood estimate of the model parameters.</li> </ul> </li> <li>Subsequently, we employ the expectation-maximization algorithm with the Gaussian mixture model assumptions to derive the Gaussian mixture model learning algorithm as a special case. <ul> <li>Lastly, in the appendix, we delve into the intricacies of the gradient computations, which underly the M-step update to the covariance matrices. Notedly, this essential step is often left to the reader.</li> </ul> </li> </ul> <p>Additionally, we note that there is an implementation of the Gaussian mixture model in the <code class="language-plaintext highlighter-rouge">ml/mixture</code> directory of the <a href="https://github.com/a-wozniakowski/ml_playground/tree/main" rel="external nofollow noopener" target="_blank">ml_playground</a> repository, which inspired a new feature in the <a href="https://github.com/scikit-learn/scikit-learn/issues/28492" rel="external nofollow noopener" target="_blank">scikit-learn</a> repository.</p> <h2>Introduction</h2> <p>Our formulation of the expectation-maximization (EM) algorithm begins with a random sample of \(m\) examples, denoted as an \(m\)-tuple:</p> <p>\begin{equation} ( X^{(1)},\dots,X^{(m)} ), \label{examples} \end{equation}</p> <p>where each \(X^{(i)} = x^{(i)} \in \mathbb{R}^{n}\) represents an independent and identically distributed random vector from an unknown probability distribution. Correspondingly, the log-likelihood \(\ell\) is then expressed as:</p> <p>\begin{equation} \ell (\theta) = \ln (f_{X_{1},\dots,X_{m}}(x^{(1)},\dots,x^{(m)})) = \ln(\prod_{i=1}^{m} f(x^{(i)})) = \sum_{i=1}^{m} \ln (f(x^{(i)})), \label{log-likelihood} \end{equation}</p> <p>where \(\theta\) bundles all model parameters, \(f_{X_{1},\dots,X_{m}}\) denotes the joint density function, and \(f_{X}=f\) denotes the population density function.</p> <p>Next, we posit a discrete random variable \(Z\) with \(k\) mutually exclusive outcomes, and we refer to the random variable as a latent variable. Subsequently, we modify the right-hand-side of Eq. \ref{log-likelihood} such that</p> <p>\begin{equation} \sum_{i=1}^{m} \ln (f(x^{(i)})) = \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j)), \label{marginalization} \end{equation}</p> <p>where \(f_{X,Z}\) denotes the joint density function.</p> <p>In turn, we seek to modify the right-hand-side of Eq. \ref{marginalization} such that we can leverage a form of Jensen’s inequality and obtain the evidence lower bound (ELBO) on the log-likelihood.</p> <h2>Evidence lower bound</h2> <p>To obtain the evidence lower bound on the log-likelihood, we begin with induction to derive a form of Jensen’s inequality. Namely, we suppose the base case states that \(g\) is a convex function</p> \[P(2):\ g(\lambda u_{1} + (1 - \lambda) u_2) \leq \lambda g(u_{1}) + (1 - \lambda) g(u_{2}),\] <p>where \(\lambda \in [ 0, 1 ].\) Next, in the induction step, we suppose the induction hypothesis states that</p> \[P(k):\ g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \leq \sum_{j=1}^{k} \lambda_{j} g(u_{j}),\] <p>where \(k \geq 2,\) \(\lambda_{j} \geq 0\) for every convex combination coefficient, and \(\sum_{j=1}^{k} \lambda_{j} = 1.\)</p> <p>Then, we have that</p> \[\begin{align*} P(k+1):\ g(\sum_{j=1}^{k+1} \lambda_{j} u_{j}) &amp;= g(\lambda_{k+1} u_{k+1} + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \frac{\lambda_{j}}{1 - \lambda_{k+1}} u_{j}) \\ &amp;= g(\lambda_{k+1} u_{k+1} + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \mu_{j} u_{j}) \\ &amp;\leq \lambda_{k+1} g(u_{k+1}) + (1 - \lambda_{k+1}) g(\sum_{j=1}^{k} \mu_{j} u_{j}) \\ &amp;\leq \lambda_{k+1} g(u_{k+1}) + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \mu_{j} g(u_{j}) \\ &amp;= \lambda_{k+1} g(u_{k+1}) + \sum_{j=1}^{k} \lambda_{j} g(u_{j}) \\ &amp;= \sum_{j=1}^{k+1} \lambda_{j} g(u_{j}) \end{align*},\] <p>where \(\mu_{j} = \frac{\lambda_{j}}{1 - \lambda_{k+1}}\) for every convex combination coefficient from \(1,\dots,k,\) the first inequality follows from the base case \(P(2),\) and the second inequality follows from the induction hypothesis \(P(k).\) Therefore, we observe that</p> \[g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \leq \sum_{j=1}^{k} \lambda_{j} g(u_{j})\] <p>for \(k \geq 2;\) and, in the case that \(g\) is a concave function, we have that</p> <p>\begin{equation} g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \geq \sum_{j=1}^{k} \lambda_{j} g(u_{j}) \label{jensen} \end{equation}</p> <p>for \(k \geq 2,\) which is the form of the inequality that we want in our derivation.</p> <p>Correspondingly, we modify the right-hand-side of Eq. \ref{marginalization} such that</p> \[\begin{align} \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j)) &amp;= \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} w_{j}^{(i)} \frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}) \label{before_jensen}\\ &amp;\geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}) \label{after_jensen} \end{align}\] <p>where the weight \(w_{j}^{(i)}\) denotes a convex combination coefficient and the inequality follows from Ineq. \ref{jensen}. That is, we have that the log-likelihood is bounded from below by the evidence lower bound</p> \[\ell (\theta) \geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}).\] <h2>Expectation-maximization algorithm</h2> <p>To derive the expectation-maximization algorithm, we initialize the model parameters such that \(\theta_{t} = \theta_{0}.\) Subsequently, in iteration \(t+1,\) we perform an E-step, then an M-step, to obtain model parameters \(\theta_{t+1},\) which monotonically increase the log-likelihood, i.e., \(\ell (\theta_{t+1}) \geq \ell (\theta_{t}).\)</p> <p>That is, initially, in iteration \(t+1,\) we seek weights such that the log-likelihood is equal to the evidence lower bound at the current model parameters.</p> <h4>E-step:</h4> <p>To find such weights, we compare the right-hand-side of Eq. \ref{before_jensen} with the right-hand-side of Ineq. \ref{after_jensen}; and from that comparison, we deduce that the quotient \(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}\) must be equal to a constant \(c.\) Accordingly, we have that</p> \[c = c \sum_{j=1}^{k} w_{j}^{(i)} = \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j) = f(x^{(i)}),\] <p>which implies that for each \(i=1,\dots,m\) and \(j=1,\dots,k\)</p> <p>\begin{equation} w_{j}^{(i)} = f_{Z \mid X}(j \mid x^{(i)} ; \theta_{t}) = \frac{f_{Z}(j ; \theta_{t}) f_{X \mid Z}(x^{(i)} \mid j ; \theta_{t})}{\sum_{u=1}^{k} f_{Z}(u ; \theta_{t}) f_{X \mid Z}(x^{(i)} \mid u ; \theta_{t})}, \label{E-step} \end{equation}</p> <p>where we make the parameterization explicit.</p> <p>Hence, we observe that the log-likelihood is equal to the evidence lower bound at the current model parameters</p> <p>\begin{equation} \ell (\theta_{t}) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(f(x^{(i)} ; \theta_{t})), \label{current-log-likelihood} \end{equation}</p> <p>when we fix the weights as in Eq. \ref{E-step}, where we, again, make the parameterization explicit. In turn, we seek model parameters, which maximally increase the log-likelihood with respect to Eq. \ref{current-log-likelihood}.</p> <h4>M-step:</h4> <p>To find such model parameters, we aim to maximize the following difference</p> \[\begin{align*} \ell (\theta) - \ell (\theta_{t}) &amp;\geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} (\ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{w_{j}^{(i)}}) - \ln(f(x^{(i)} ; \theta_{t}))) \\ &amp;= \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})}) \end{align*}.\] <p>Correspondingly, we modify the inequality such that</p> \[\begin{align*} \ell (\theta) &amp;\geq \ell (\theta_{t}) + \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})}) \\ &amp; = \ell (\theta_{t}) + S(\theta, \theta_{t}) \\ &amp; = T(\theta, \theta_{t}), \end{align*}\] <p>where we introduce functions</p> \[S(\theta, \theta_{t}) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})})\] <p>and</p> \[T(\theta, \theta_{t}) = \ell (\theta_{t}) + S(\theta, \theta_{t})\] <p>which satisfy \(S(\theta_{t}, \theta_{t}) = 0\) and \(T(\theta_{t}, \theta_{t}) = \ell (\theta_{t}).\)</p> <p>Thus, if we find \(\theta\) such that</p> \[T(\theta, \theta_{t}) \geq T(\theta_{t}, \theta_{t}),\] <p>then we have that</p> \[\ell (\theta) \geq T(\theta, \theta_{t}) \geq T(\theta_{t}, \theta_{t}) = \ell (\theta_{t}),\] <p>which implies that we monotonicly increase the log-likelihood with respect to Eq. \ref{current-log-likelihood}. Trivially, to do so, we pick</p> \[\theta_{t+1} = \theta = \theta_{t},\] <p>where we saturate the inequality.</p> <p>However, our goal is to maximally increase the log-likelihood with respect to Eq. \ref{current-log-likelihood}. Therefore, we must solve the following optimization problem to obtain the desired model parameters</p> <p>\begin{equation} \theta_{t+1} = \underset{\theta}{\operatorname{argmax}} T(\theta, \theta_{t}) = \underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (f_{X,Z}(x^{(i)}, j ; \theta)). \label{M-step} \end{equation}</p> <p>Subsequently, we continue to iterate, where we alternate between the E-step and M-step until we reach some convergence criterion, i.e., we find the maximum likelihood estimate of the model parameters.</p> <h2>Gaussian mixture model</h2> <p>To derive the Gaussian mixture model, we assume that the latent variable</p> \[Z \sim Categorical (\phi),\] <p>where a categorical random variable is a special case of a multinomial random variable with a single trial and \(k\) mutually exclusive outcomes. Moreover, the categorical random variable has probability mass function</p> \[f_{Z}(z) = \prod_{j=1}^{k} \phi_{j}^{\mathcal{I} \{ z=j \}},\] <p>where \(\mathcal{I}\) denotes the indicator function. Next, we assume that given the latent variable \(Z=j,\) we have that</p> \[X | Z = j \sim \mathcal{N}(\mu_{j}, \Sigma_{j}),\] <p>where the conditional Gaussian random vector has conditional probability density function</p> \[f_{X|Z} (x | j) = \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{j}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_{j})^{t}\Sigma_{j}^{-1}(x-\mu_{j})}.\] <p>Correspondingly, we initialize all model parameters, then we consider iteration \(t+1.\)</p> <h4>E-step:</h4> <p>Namely, we set the weights such that</p> \[w_{j}^{(i)} = \frac{\phi_{j} \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{j}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{j})^{t}\Sigma_{j}^{-1}(x^{(i)}-\mu_{j})}}{\sum_{u=1}^{k} \phi_{u} \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}}\] <p>for each \(i=1,\dots,m\) and \(j=1,\dots,k,\) which follows from Eq. \ref{E-step}.</p> <h4>M-step:</h4> <p>Then, we set the model parameters such that they satisfy Eq. \ref{M-step}. To do so, in the case of \(\phi,\) we posit the auxillary function</p> \[h(\phi_{1},\dots,\phi_{k},\lambda) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1),\] <p>where \(\lambda\) denotes the Lagrange multiplier and we only include the terms in Eq. \ref{M-step}, which depend on \(\phi_{1},\dots,\phi_{k}.\) Subsequently, we apply the Lagrange multiplier theorem to obtain</p> \[\begin{align*} 0 &amp;= \frac{\partial}{\partial \phi_{1}} (\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1)) = \sum_{i=1}^{m} w_{1}^{(i)} \frac{1}{\phi_{1}} - \lambda\\ &amp;\vdots \\ 0 &amp;= \frac{\partial}{\partial \phi_{k}} (\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1)) = \sum_{i=1}^{m} w_{k}^{(i)} \frac{1}{\phi_{k}} - \lambda \\ 0 &amp;= \frac{\partial}{\partial \lambda} \lambda (\sum_{j=1}^{k} \phi_{j} - 1) = \sum_{j=1}^{k} \phi_{j} - 1 \end{align*}\] <p>which implies that</p> \[m = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} = \lambda \sum_{j=1}^{k} \phi_{j} = \lambda.\] <p>Therefore, we have that</p> \[\hat{\phi}_{j} = \frac{\sum_{i=1}^{m} w_{j}^{(i)}}{m}\] <p>for \(j=1,\dots,k.\)</p> <p>To do so, in the case of \(\mu_{j},\) we consider the critical point of the gradient of the summand in Eq. \ref{M-step} at the \(j\)th mean</p> \[0 = \nabla_{\mu_{j}} \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \ln(\frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}),\] <p>where we only include the terms in the summand, which depend on \(\mu_{j}.\) Subsequently, we simplify the equation to obtain</p> \[\begin{align*} 0 &amp;= \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \nabla_{\mu_{j}} -\frac{1}{2} (x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u}) \\ &amp;= \Sigma_{j}^{-1} \sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j}) \\ \end{align*}\] <p>which implies that</p> \[0 = \sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j})\] <p>since the \(j\)th covariance matrix is invertible. Therefore, we have that</p> \[\hat{\mu}_{j} = \frac{\sum_{i=1}^{m} w^{(i)}_{j} x^{(i)}}{\sum_{i=1}^{m} w^{(i)}_{j}}\] <p>for \(j=1,\dots,k.\)</p> <p>To do so, in the case of \(\Sigma_{j},\) we consider the critical point of the gradient of the summand in Eq. \ref{M-step} at the \(j\)th covariance matrix</p> \[0 = \nabla_{\Sigma_{j}} \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \ln(\frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}),\] <p>where we only include the terms in the summand, which depend on \(\Sigma_{j}.\) Subsequently, we simplify the equation to obtain</p> \[0 = \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \nabla_{\Sigma_{j}} -\frac{1}{2} (\ln(|\Sigma_{u}|) + (x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})),\] <p>which implies that</p> \[\begin{align*} 0 &amp;= \sum_{i=1}^{m} \sum_{u=1}^{k} \mathcal{I} \{ j=u \} w^{(i)}_{u} (\Sigma_{u}^{-1} - \Sigma_{u}^{-1} (x^{(i)} - \mu_{u}) (x^{(i)} - \mu_{u})^{t} \Sigma_{u}^{-1}) \\ &amp;= \sum_{i=1}^{m} w^{(i)}_{j} (\Sigma_{j}^{-1} - \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t} \Sigma_{j}^{-1}) \end{align*},\] <p>where we employ Eq. \ref{gradient-determinant} and Eq. \ref{gradient-quadratic-form} from the Appendix to perform the gradient computations.</p> <p>Then, we left and right multiply by the \(j\)th covariance matrix to obtain</p> \[0 = \sum_{i=1}^{m} w^{(i)}_{j} (\Sigma_{j} - (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t}),\] <p>Therefore, we have that</p> \[\hat{\Sigma}_{j} = \frac{\sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t}}{\sum_{i=1}^{m} w^{(i)}_{j}}\] <p>for \(j=1,\dots,k,\) which concludes the derivation of the Gaussian mixture model learning algorithm.</p> <p>Below, we delve into the intricacies of the gradient computations in Eq. \ref{gradient-determinant} and Eq. \ref{gradient-quadratic-form}, which underly the update to the covariance matrices.</p> <h2>Appendix: Gradient computations</h2> <p>First, we want to show that</p> <p>\begin{equation} \nabla_{A} \ln(|A|) = A^{-1}, \label{gradient-determinant} \end{equation}</p> <p>where \(A \in M_{n \times n} (\mathbb{R})\) denotes an invertible symmetric matrix. To do so, we begin with</p> \[\frac{\partial}{\partial A_{ij}} \ln(|A|) = \frac{1}{|A|} \frac{\partial}{\partial A_{ij}} |A| = \frac{1}{|A|} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} A_{ik} C_{ik},\] <p>where we employ a cofactor expansion of the determinant on the right-hand-side of the equation. Then, we have that</p> \[\frac{1}{|A|} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} A_{ik} C_{ik} = \frac{C_{ij}}{|A|} = (\frac{C^{t}}{|A|})_{ji} = (A^{-1})_{ji} = ((A^{-1})^{t})_{ij} = ((A^{t})^{-1})_{ij},\] <p>which implies that</p> \[\nabla_{A} \ln(|A|) = (A^{t})^{-1}.\] <p>Therefore, we obtain Eq. \ref{gradient-determinant}, since \(A=A^{t}.\)</p> <p>Second, we want to show that</p> <p>\begin{equation} \nabla_{A} y^{t} A^{-1} y = -A^{-1} y y^{t} A^{-1} \label{gradient-quadratic-form} \end{equation}</p> <p>where \(A \in M_{n \times n} (\mathbb{R})\) denotes an invertible symmetric matrix and \(y \in \mathbb{R}^{n}\) denotes the corresponding vector. To do so, we begin with</p> \[0 = \frac{\partial}{\partial A_{ij}} I_{uv} = \frac{\partial}{\partial A_{ij}} (A^{-1}A)_{uv} = \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} (A^{-1})_{uk} A_{kv},\] <p>where \(I\) denotes the \(n \times n\) identity matrix. Then, we have that</p> \[\begin{align*} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} (A^{-1})_{uk} A_{kv} &amp;= \sum_{k=1}^{n} \frac{\partial}{\partial A_{ij}} (A^{-1})_{uk} A_{kv} + \sum_{k=1}^{n} (A^{-1})_{uk} \frac{\partial}{\partial A_{ij}} A_{kv}\\ &amp;= \sum_{k=1}^{n} (\frac{\partial A^{-1}}{\partial A_{ij}})_{uk} A_{kv} + \sum_{k=1}^{n} (A^{-1})_{uk} (E_{ij})_{kv} \\ &amp;= (\frac{\partial A^{-1}}{\partial A_{ij}} A)_{uv} + (A^{-1} E_{ij})_{uv} \end{align*}\] <p>since</p> \[\frac{\partial}{\partial A_{ij}} A_{kv} = \mathcal{I} \{ i=k \} \mathcal{I} \{ j=v \} = (E_{ij})_{kv},\] <p>where \(E_{ij}\) denotes the \(n \times n\) matrix unit with a \(1\) in the \((i,j)\) entry and \(0\) in every other entry. Hence, we have that</p> \[(\frac{\partial A^{-1}}{\partial A_{ij}} A)_{uv} = -(A^{-1} E_{ij})_{uv},\] <p>which implies that</p> \[\frac{\partial A^{-1}}{\partial A_{ij}} = - A^{-1} E_{ij} A^{-1}.\] <p>In turn, we consider</p> \[\frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} = (\frac{\partial A^{-1}}{\partial A_{ij}})_{kl} = - (A^{-1} E_{ij} A^{-1})_{kl} = - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} (E_{ij})_{uv} (A^{-1})_{vl}.\] <p>Then, we have that</p> \[\begin{align*} - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} (E_{ij})_{uv} (A^{-1})_{vl} &amp;= - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} \mathcal{I} \{ i=u \} \mathcal{I} \{ j=v \} (A^{-1})_{vl} \\ &amp;= - (A^{-1})_{ki} (A^{-1})_{jl} \end{align*},\] <p>which implies that</p> \[\begin{equation} \frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} = - (A^{-1})_{ki} (A^{-1})_{jl}. \label{partial-derivative} \end{equation}\] <p>Lastly, we consider</p> \[\frac{\partial}{\partial A_{ij}} y^{t} A^{-1} y = \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} \frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} y_{l} = - \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} (A^{-1})_{ki} (A^{-1})_{jl} y_{l},\] <p>which follows from Eq. \ref{partial-derivative}. Consequently, we have that</p> \[- \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} (A^{-1})_{ki} (A^{-1})_{jl} y_{l} = - ((A^{-1})^{t} y)_{i} (A^{-1} y)_{j},\] <p>which implies that</p> \[\nabla_{A} y^{t} A^{-1} y = - (A^{-1})^{t} y y^{t} (A^{-1})^{t} = - (A^{t})^{-1} y y^{t} (A^{t})^{-1}.\] <p>Therefore, we obtain Eq. \ref{gradient-quadratic-form}, since \(A=A^{t}.\)</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Alex Wozniakowski. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>