<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://a-wozniakowski.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://a-wozniakowski.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-17T19:55:23+00:00</updated><id>https://a-wozniakowski.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Gaussian mixture model</title><link href="https://a-wozniakowski.github.io/blog/2024/gmm/" rel="alternate" type="text/html" title="Gaussian mixture model"/><published>2024-03-17T00:00:00+00:00</published><updated>2024-03-17T00:00:00+00:00</updated><id>https://a-wozniakowski.github.io/blog/2024/gmm</id><content type="html" xml:base="https://a-wozniakowski.github.io/blog/2024/gmm/"><![CDATA[<p>In this exposition, we formulate the expectation-maximization (EM) algorithm and employ it to derive the Gaussian mixture model (GMM).</p> <ul> <li>To achieve this, we prove a form of Jensen’s inequality via induction and apply it to obtain the evidence lower bound (ELBO), where the latter plays a broad role in modern machine learning.</li> <li>In turn, we see how the interplay between the log-likelihood and the evidence lower bound naturally leads us to a derivation of the expectation-maximization algorithm: <ul> <li>A point estimation method that initializes model parameters then alternates between an E-step and M-step until convergence to the maximum likelihood estimate of the model parameters.</li> </ul> </li> <li>Subsequently, we employ the expectation-maximization algorithm with the Gaussian mixture model assumptions to derive the Gaussian mixture model learning algorithm as a special case. <ul> <li>Lastly, in the appendix, we delve into the intricacies of the gradient computations, which underly the M-step update to the covariance matrices. Notedly, this essential step is often left to the reader.</li> </ul> </li> </ul> <p>Additionally, we note that there is an implementation of the Gaussian mixture model in the <code class="language-plaintext highlighter-rouge">ml/mixture</code> directory of the <a href="https://github.com/a-wozniakowski/ml_playground/tree/main">ml_playground</a> repository, which inspired a new feature in the <a href="https://github.com/scikit-learn/scikit-learn/issues/28492">scikit-learn</a> repository.</p> <h2>Introduction</h2> <p>Our formulation of the expectation-maximization (EM) algorithm begins with a random sample of \(m\) examples, denoted as an \(m\)-tuple:</p> <p>\begin{equation} ( X^{(1)},\dots,X^{(m)} ), \label{examples} \end{equation}</p> <p>where each \(X^{(i)} = x^{(i)} \in \mathbb{R}^{n}\) represents an independent and identically distributed random vector from an unknown probability distribution. Correspondingly, the log-likelihood \(\ell\) is then expressed as:</p> <p>\begin{equation} \ell (\theta) = \ln (f_{X_{1},\dots,X_{m}}(x^{(1)},\dots,x^{(m)})) = \ln(\prod_{i=1}^{m} f(x^{(i)})) = \sum_{i=1}^{m} \ln (f(x^{(i)})), \label{log-likelihood} \end{equation}</p> <p>where \(\theta\) bundles all model parameters, \(f_{X_{1},\dots,X_{m}}\) denotes the joint density function, and \(f_{X}=f\) denotes the population density function.</p> <p>Next, we posit a discrete random variable \(Z\) with \(k\) mutually exclusive outcomes, and we refer to the random variable as a latent variable. Subsequently, we modify the right-hand-side of Eq. \ref{log-likelihood} such that</p> <p>\begin{equation} \sum_{i=1}^{m} \ln (f(x^{(i)})) = \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j)), \label{marginalization} \end{equation}</p> <p>where \(f_{X,Z}\) denotes the joint density function.</p> <p>In turn, we seek to modify the right-hand-side of Eq. \ref{marginalization} such that we can leverage a form of Jensen’s inequality and obtain the evidence lower bound (ELBO) on the log-likelihood.</p> <h2>Evidence lower bound</h2> <p>To obtain the evidence lower bound on the log-likelihood, we begin with induction to derive a form of Jensen’s inequality. Namely, we suppose the base case states that \(g\) is a convex function</p> \[P(2):\ g(\lambda u_{1} + (1 - \lambda) u_2) \leq \lambda g(u_{1}) + (1 - \lambda) g(u_{2}),\] <p>where \(\lambda \in [ 0, 1 ].\) Next, in the induction step, we suppose the induction hypothesis states that</p> \[P(k):\ g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \leq \sum_{j=1}^{k} \lambda_{j} g(u_{j}),\] <p>where \(k \geq 2,\) \(\lambda_{j} \geq 0\) for every convex combination coefficient, and \(\sum_{j=1}^{k} \lambda_{j} = 1.\)</p> <p>Then, we have that</p> \[\begin{align*} P(k+1):\ g(\sum_{j=1}^{k+1} \lambda_{j} u_{j}) &amp;= g(\lambda_{k+1} u_{k+1} + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \frac{\lambda_{j}}{1 - \lambda_{k+1}} u_{j}) \\ &amp;= g(\lambda_{k+1} u_{k+1} + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \mu_{j} u_{j}) \\ &amp;\leq \lambda_{k+1} g(u_{k+1}) + (1 - \lambda_{k+1}) g(\sum_{j=1}^{k} \mu_{j} u_{j}) \\ &amp;\leq \lambda_{k+1} g(u_{k+1}) + (1 - \lambda_{k+1}) \sum_{j=1}^{k} \mu_{j} g(u_{j}) \\ &amp;= \lambda_{k+1} g(u_{k+1}) + \sum_{j=1}^{k} \lambda_{j} g(u_{j}) \\ &amp;= \sum_{j=1}^{k+1} \lambda_{j} g(u_{j}) \end{align*},\] <p>where \(\mu_{j} = \frac{\lambda_{j}}{1 - \lambda_{k+1}}\) for every convex combination coefficient from \(1,\dots,k,\) the first inequality follows from the base case \(P(2),\) and the second inequality follows from the induction hypothesis \(P(k).\) Therefore, we observe that</p> \[g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \leq \sum_{j=1}^{k} \lambda_{j} g(u_{j})\] <p>for \(k \geq 2;\) and, in the case that \(g\) is a concave function, we have that</p> <p>\begin{equation} g(\sum_{j=1}^{k} \lambda_{j} u_{j}) \geq \sum_{j=1}^{k} \lambda_{j} g(u_{j}) \label{jensen} \end{equation}</p> <p>for \(k \geq 2,\) which is the form of the inequality that we want in our derivation.</p> <p>Correspondingly, we modify the right-hand-side of Eq. \ref{marginalization} such that</p> \[\begin{align} \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j)) &amp;= \sum_{i=1}^{m} \ln ( \sum_{j=1}^{k} w_{j}^{(i)} \frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}) \label{before_jensen}\\ &amp;\geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}) \label{after_jensen} \end{align}\] <p>where the weight \(w_{j}^{(i)}\) denotes a convex combination coefficient and the inequality follows from Ineq. \ref{jensen}. That is, we have that the log-likelihood is bounded from below by the evidence lower bound</p> \[\ell (\theta) \geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}).\] <h2>Expectation-maximization algorithm</h2> <p>To derive the expectation-maximization algorithm, we initialize the model parameters such that \(\theta_{t} = \theta_{0}.\) Subsequently, in iteration \(t+1,\) we perform an E-step, then an M-step, to obtain model parameters \(\theta_{t+1},\) which monotonically increase the log-likelihood, i.e., \(\ell (\theta_{t+1}) \geq \ell (\theta_{t}).\)</p> <p>That is, initially, in iteration \(t+1,\) we seek weights such that the log-likelihood is equal to the evidence lower bound at the current model parameters.</p> <h4>E-step:</h4> <p>To find such weights, we compare the right-hand-side of Eq. \ref{before_jensen} with the right-hand-side of Ineq. \ref{after_jensen}; and from that comparison, we deduce that the quotient \(\frac{f_{X, Z}(x^{(i)}, j)}{w_{j}^{(i)}}\) must be equal to a constant \(c.\) Accordingly, we have that</p> \[c = c \sum_{j=1}^{k} w_{j}^{(i)} = \sum_{j=1}^{k} f_{X, Z}(x^{(i)}, j) = f(x^{(i)}),\] <p>which implies that for each \(i=1,\dots,m\) and \(j=1,\dots,k\)</p> <p>\begin{equation} w_{j}^{(i)} = f_{Z \mid X}(j \mid x^{(i)} ; \theta_{t}) = \frac{f_{Z}(j ; \theta_{t}) f_{X \mid Z}(x^{(i)} \mid j ; \theta_{t})}{\sum_{u=1}^{k} f_{Z}(u ; \theta_{t}) f_{X \mid Z}(x^{(i)} \mid u ; \theta_{t})}, \label{E-step} \end{equation}</p> <p>where we make the parameterization explicit.</p> <p>Hence, we observe that the log-likelihood is equal to the evidence lower bound at the current model parameters</p> <p>\begin{equation} \ell (\theta_{t}) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(f(x^{(i)} ; \theta_{t})), \label{current-log-likelihood} \end{equation}</p> <p>when we fix the weights as in Eq. \ref{E-step}, where we, again, make the parameterization explicit. In turn, we seek model parameters, which maximally increase the log-likelihood with respect to Eq. \ref{current-log-likelihood}.</p> <h4>M-step:</h4> <p>To find such model parameters, we aim to maximize the following difference</p> \[\begin{align*} \ell (\theta) - \ell (\theta_{t}) &amp;\geq \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} (\ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{w_{j}^{(i)}}) - \ln(f(x^{(i)} ; \theta_{t}))) \\ &amp;= \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})}) \end{align*}.\] <p>Correspondingly, we modify the inequality such that</p> \[\begin{align*} \ell (\theta) &amp;\geq \ell (\theta_{t}) + \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})}) \\ &amp; = \ell (\theta_{t}) + S(\theta, \theta_{t}) \\ &amp; = T(\theta, \theta_{t}), \end{align*}\] <p>where we introduce functions</p> \[S(\theta, \theta_{t}) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln(\frac{f_{X, Z}(x^{(i)}, j ; \theta)}{f_{X, Z}(x^{(i)}, j ; \theta_{t})})\] <p>and</p> \[T(\theta, \theta_{t}) = \ell (\theta_{t}) + S(\theta, \theta_{t})\] <p>which satisfy \(S(\theta_{t}, \theta_{t}) = 0\) and \(T(\theta_{t}, \theta_{t}) = \ell (\theta_{t}).\)</p> <p>Thus, if we find \(\theta\) such that</p> \[T(\theta, \theta_{t}) \geq T(\theta_{t}, \theta_{t}),\] <p>then we have that</p> \[\ell (\theta) \geq T(\theta, \theta_{t}) \geq T(\theta_{t}, \theta_{t}) = \ell (\theta_{t}),\] <p>which implies that we monotonicly increase the log-likelihood with respect to Eq. \ref{current-log-likelihood}. Trivially, to do so, we pick</p> \[\theta_{t+1} = \theta = \theta_{t},\] <p>where we saturate the inequality.</p> <p>However, our goal is to maximally increase the log-likelihood with respect to Eq. \ref{current-log-likelihood}. Therefore, we must solve the following optimization problem to obtain the desired model parameters</p> <p>\begin{equation} \theta_{t+1} = \underset{\theta}{\operatorname{argmax}} T(\theta, \theta_{t}) = \underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (f_{X,Z}(x^{(i)}, j ; \theta)). \label{M-step} \end{equation}</p> <p>Subsequently, we continue to iterate, where we alternate between the E-step and M-step until we reach some convergence criterion, i.e., we find the maximum likelihood estimate of the model parameters.</p> <h2>Gaussian mixture model</h2> <p>To derive the Gaussian mixture model, we assume that the latent variable</p> \[Z \sim Categorical (\phi),\] <p>where a categorical random variable is a special case of a multinomial random variable with a single trial and \(k\) mutually exclusive outcomes. Moreover, the categorical random variable has probability mass function</p> \[f_{Z}(z) = \prod_{j=1}^{k} \phi_{j}^{\mathcal{I} \{ z=j \}},\] <p>where \(\mathcal{I}\) denotes the indicator function. Next, we assume that given the latent variable \(Z=j,\) we have that</p> \[X | Z = j \sim \mathcal{N}(\mu_{j}, \Sigma_{j}),\] <p>where the conditional Gaussian random vector has conditional probability density function</p> \[f_{X|Z} (x | j) = \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{j}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu_{j})^{t}\Sigma_{j}^{-1}(x-\mu_{j})}.\] <p>Correspondingly, we initialize all model parameters, then we consider iteration \(t+1.\)</p> <h4>E-step:</h4> <p>Namely, we set the weights such that</p> \[w_{j}^{(i)} = \frac{\phi_{j} \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{j}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{j})^{t}\Sigma_{j}^{-1}(x^{(i)}-\mu_{j})}}{\sum_{u=1}^{k} \phi_{u} \frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}}\] <p>for each \(i=1,\dots,m\) and \(j=1,\dots,k,\) which follows from Eq. \ref{E-step}.</p> <h4>M-step:</h4> <p>Then, we set the model parameters such that they satisfy Eq. \ref{M-step}. To do so, in the case of \(\phi,\) we posit the auxillary function</p> \[h(\phi_{1},\dots,\phi_{k},\lambda) = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1),\] <p>where \(\lambda\) denotes the Lagrange multiplier and we only include the terms in Eq. \ref{M-step}, which depend on \(\phi_{1},\dots,\phi_{k}.\) Subsequently, we apply the Lagrange multiplier theorem to obtain</p> \[\begin{align*} 0 &amp;= \frac{\partial}{\partial \phi_{1}} (\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1)) = \sum_{i=1}^{m} w_{1}^{(i)} \frac{1}{\phi_{1}} - \lambda\\ &amp;\vdots \\ 0 &amp;= \frac{\partial}{\partial \phi_{k}} (\sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} \ln (\phi_{j}) - \lambda (\sum_{j=1}^{k} \phi_{j} - 1)) = \sum_{i=1}^{m} w_{k}^{(i)} \frac{1}{\phi_{k}} - \lambda \\ 0 &amp;= \frac{\partial}{\partial \lambda} \lambda (\sum_{j=1}^{k} \phi_{j} - 1) = \sum_{j=1}^{k} \phi_{j} - 1 \end{align*}\] <p>which implies that</p> \[m = \sum_{i=1}^{m} \sum_{j=1}^{k} w_{j}^{(i)} = \lambda \sum_{j=1}^{k} \phi_{j} = \lambda.\] <p>Therefore, we have that</p> \[\hat{\phi}_{j} = \frac{\sum_{i=1}^{m} w_{j}^{(i)}}{m}\] <p>for \(j=1,\dots,k.\)</p> <p>To do so, in the case of \(\mu_{j},\) we consider the critical point of the gradient of the summand in Eq. \ref{M-step} at the \(j\)th mean</p> \[0 = \nabla_{\mu_{j}} \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \ln(\frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}),\] <p>where we only include the terms in the summand, which depend on \(\mu_{j}.\) Subsequently, we simplify the equation to obtain</p> \[\begin{align*} 0 &amp;= \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \nabla_{\mu_{j}} -\frac{1}{2} (x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u}) \\ &amp;= \Sigma_{j}^{-1} \sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j}) \\ \end{align*}\] <p>which implies that</p> \[0 = \sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j})\] <p>since the \(j\)th covariance matrix is invertible. Therefore, we have that</p> \[\hat{\mu}_{j} = \frac{\sum_{i=1}^{m} w^{(i)}_{j} x^{(i)}}{\sum_{i=1}^{m} w^{(i)}_{j}}\] <p>for \(j=1,\dots,k.\)</p> <p>To do so, in the case of \(\Sigma_{j},\) we consider the critical point of the gradient of the summand in Eq. \ref{M-step} at the \(j\)th covariance matrix</p> \[0 = \nabla_{\Sigma_{j}} \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \ln(\frac{1}{(2 \pi)^{\frac{n}{2}} |\Sigma_{u}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})}),\] <p>where we only include the terms in the summand, which depend on \(\Sigma_{j}.\) Subsequently, we simplify the equation to obtain</p> \[0 = \sum_{i=1}^{m} \sum_{u=1}^{k} w^{(i)}_{u} \nabla_{\Sigma_{j}} -\frac{1}{2} (\ln(|\Sigma_{u}|) + (x^{(i)}-\mu_{u})^{t}\Sigma_{u}^{-1}(x^{(i)}-\mu_{u})),\] <p>which implies that</p> \[\begin{align*} 0 &amp;= \sum_{i=1}^{m} \sum_{u=1}^{k} \mathcal{I} \{ j=u \} w^{(i)}_{u} (\Sigma_{u}^{-1} - \Sigma_{u}^{-1} (x^{(i)} - \mu_{u}) (x^{(i)} - \mu_{u})^{t} \Sigma_{u}^{-1}) \\ &amp;= \sum_{i=1}^{m} w^{(i)}_{j} (\Sigma_{j}^{-1} - \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t} \Sigma_{j}^{-1}) \end{align*},\] <p>where we employ Eq. \ref{gradient-determinant} and Eq. \ref{gradient-quadratic-form} from the Appendix to perform the gradient computations.</p> <p>Then, we left and right multiply by the \(j\)th covariance matrix to obtain</p> \[0 = \sum_{i=1}^{m} w^{(i)}_{j} (\Sigma_{j} - (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t}),\] <p>Therefore, we have that</p> \[\hat{\Sigma}_{j} = \frac{\sum_{i=1}^{m} w^{(i)}_{j} (x^{(i)} - \mu_{j}) (x^{(i)} - \mu_{j})^{t}}{\sum_{i=1}^{m} w^{(i)}_{j}}\] <p>for \(j=1,\dots,k,\) which concludes the derivation of the Gaussian mixture model learning algorithm.</p> <p>Below, we delve into the intricacies of the gradient computations in Eq. \ref{gradient-determinant} and Eq. \ref{gradient-quadratic-form}, which underly the update to the covariance matrices.</p> <h2>Appendix: Gradient computations</h2> <p>First, we want to show that</p> <p>\begin{equation} \nabla_{A} \ln(|A|) = A^{-1}, \label{gradient-determinant} \end{equation}</p> <p>where \(A \in M_{n \times n} (\mathbb{R})\) denotes an invertible symmetric matrix. To do so, we begin with</p> \[\frac{\partial}{\partial A_{ij}} \ln(|A|) = \frac{1}{|A|} \frac{\partial}{\partial A_{ij}} |A| = \frac{1}{|A|} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} A_{ik} C_{ik},\] <p>where we employ a cofactor expansion of the determinant on the right-hand-side of the equation. Then, we have that</p> \[\frac{1}{|A|} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} A_{ik} C_{ik} = \frac{C_{ij}}{|A|} = (\frac{C^{t}}{|A|})_{ji} = (A^{-1})_{ji} = ((A^{-1})^{t})_{ij} = ((A^{t})^{-1})_{ij},\] <p>which implies that</p> \[\nabla_{A} \ln(|A|) = (A^{t})^{-1}.\] <p>Therefore, we obtain Eq. \ref{gradient-determinant}, since \(A=A^{t}.\)</p> <p>Second, we want to show that</p> <p>\begin{equation} \nabla_{A} y^{t} A^{-1} y = -A^{-1} y y^{t} A^{-1} \label{gradient-quadratic-form} \end{equation}</p> <p>where \(A \in M_{n \times n} (\mathbb{R})\) denotes an invertible symmetric matrix and \(y \in \mathbb{R}^{n}\) denotes the corresponding vector. To do so, we begin with</p> \[0 = \frac{\partial}{\partial A_{ij}} I_{uv} = \frac{\partial}{\partial A_{ij}} (A^{-1}A)_{uv} = \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} (A^{-1})_{uk} A_{kv},\] <p>where \(I\) denotes the \(n \times n\) identity matrix. Then, we have that</p> \[\begin{align*} \frac{\partial}{\partial A_{ij}} \sum_{k=1}^{n} (A^{-1})_{uk} A_{kv} &amp;= \sum_{k=1}^{n} \frac{\partial}{\partial A_{ij}} (A^{-1})_{uk} A_{kv} + \sum_{k=1}^{n} (A^{-1})_{uk} \frac{\partial}{\partial A_{ij}} A_{kv}\\ &amp;= \sum_{k=1}^{n} (\frac{\partial A^{-1}}{\partial A_{ij}})_{uk} A_{kv} + \sum_{k=1}^{n} (A^{-1})_{uk} (E_{ij})_{kv} \\ &amp;= (\frac{\partial A^{-1}}{\partial A_{ij}} A)_{uv} + (A^{-1} E_{ij})_{uv} \end{align*}\] <p>since</p> \[\frac{\partial}{\partial A_{ij}} A_{kv} = \mathcal{I} \{ i=k \} \mathcal{I} \{ j=v \} = (E_{ij})_{kv},\] <p>where \(E_{ij}\) denotes the \(n \times n\) matrix unit with a \(1\) in the \((i,j)\) entry and \(0\) in every other entry. Hence, we have that</p> \[(\frac{\partial A^{-1}}{\partial A_{ij}} A)_{uv} = -(A^{-1} E_{ij})_{uv},\] <p>which implies that</p> \[\frac{\partial A^{-1}}{\partial A_{ij}} = - A^{-1} E_{ij} A^{-1}.\] <p>In turn, we consider</p> \[\frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} = (\frac{\partial A^{-1}}{\partial A_{ij}})_{kl} = - (A^{-1} E_{ij} A^{-1})_{kl} = - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} (E_{ij})_{uv} (A^{-1})_{vl}.\] <p>Then, we have that</p> \[\begin{align*} - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} (E_{ij})_{uv} (A^{-1})_{vl} &amp;= - \sum_{u=1}^{n} \sum_{v=1}^{n} (A^{-1})_{ku} \mathcal{I} \{ i=u \} \mathcal{I} \{ j=v \} (A^{-1})_{vl} \\ &amp;= - (A^{-1})_{ki} (A^{-1})_{jl} \end{align*},\] <p>which implies that</p> \[\begin{equation} \frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} = - (A^{-1})_{ki} (A^{-1})_{jl}. \label{partial-derivative} \end{equation}\] <p>Lastly, we consider</p> \[\frac{\partial}{\partial A_{ij}} y^{t} A^{-1} y = \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} \frac{\partial}{\partial A_{ij}} (A^{-1})_{kl} y_{l} = - \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} (A^{-1})_{ki} (A^{-1})_{jl} y_{l},\] <p>which follows from Eq. \ref{partial-derivative}. Consequently, we have that</p> \[- \sum_{k=1}^{n} \sum_{l=1}^{n} y_{k} (A^{-1})_{ki} (A^{-1})_{jl} y_{l} = - ((A^{-1})^{t} y)_{i} (A^{-1} y)_{j},\] <p>which implies that</p> \[\nabla_{A} y^{t} A^{-1} y = - (A^{-1})^{t} y y^{t} (A^{-1})^{t} = - (A^{t})^{-1} y y^{t} (A^{t})^{-1}.\] <p>Therefore, we obtain Eq. \ref{gradient-quadratic-form}, since \(A=A^{t}.\)</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="statistics"/><category term="expectation-maximization"/><category term="EM"/><category term="gaussian-mixture-model"/><category term="GMM"/><summary type="html"><![CDATA[Derivation of a Gaussian mixture model]]></summary></entry></feed>